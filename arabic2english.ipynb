{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c92885db",
   "metadata": {},
   "source": [
    "# Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e746c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#to open csv file\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#sentences & words tokenization\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#regular expression \n",
    "import re\n",
    "#for stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "# from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.stem import ISRIStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feb6d176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hends\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hends\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hends\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for preprocessing\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f712df6",
   "metadata": {},
   "source": [
    "# csv path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc8806c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arabic</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>متى أنشئت هذه الجامعة؟</td>\n",
       "      <td>When was this university founded?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>أراها نادراً</td>\n",
       "      <td>I see it rarely.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>يعزف على البيانو بشكل جيد جداً</td>\n",
       "      <td>He plays the piano very well.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>مع كل احترامي.</td>\n",
       "      <td>With all due respect.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>نظف أسنانك</td>\n",
       "      <td>Brush your teeth clean.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           arabic                            english\n",
       "0          متى أنشئت هذه الجامعة؟  When was this university founded?\n",
       "1                    أراها نادراً                   I see it rarely.\n",
       "2  يعزف على البيانو بشكل جيد جداً      He plays the piano very well.\n",
       "3                  مع كل احترامي.              With all due respect.\n",
       "4                      نظف أسنانك            Brush your teeth clean."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = r'C:\\Users\\hends\\Documents\\ANLP\\dataset\\arabic_english.csv'\n",
    "dataset = pd.read_csv(dataset_path, encoding=\"utf-8\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a7056",
   "metadata": {},
   "source": [
    "# preprocessing function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf6fe42",
   "metadata": {},
   "source": [
    "### arabic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f109601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Apply lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove Arabic stopwords\n",
    "    stop_words = set(stopwords.words('arabic'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Remove Arabic punctuation and other non-alphanumeric characters\n",
    "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
    "    # Remove empty tokens\n",
    "    tokens = [token for token in tokens if token]\n",
    "    # Apply lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Apply stemming\n",
    "    stemmer = ISRIStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d98d0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['arabic_preprocessed'] = dataset['arabic'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c1cd216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       [شئت, جمع]\n",
       "1                       [ارا, ندر]\n",
       "2        [عزف, ينو, شكل, جيد, جدا]\n",
       "3                            [حرم]\n",
       "4                       [نظف, سنن]\n",
       "                   ...            \n",
       "34884    [ذهب, انت, يسر, ونأ, يمي]\n",
       "34885         [يجب, قرء, لنص, بتأ]\n",
       "34886                   [عند, شكل]\n",
       "34887                   [عند, شكل]\n",
       "34888                   [عند, شكل]\n",
       "Name: arabic_preprocessed, Length: 34889, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['arabic_preprocessed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e09b2",
   "metadata": {},
   "source": [
    "### english preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da50341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_english_text(text):\n",
    "    # Apply lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Remove English punctuation and other non-alphanumeric characters\n",
    "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]\n",
    "    # Remove empty tokens\n",
    "    tokens = [token for token in tokens if token]\n",
    "    # Apply lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5176fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['english_preprocessed'] = dataset['english'].apply(preprocess_english_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "579ef3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  [university, founded]\n",
       "1                          [see, rarely]\n",
       "2                    [play, piano, well]\n",
       "3                         [due, respect]\n",
       "4                  [brush, teeth, clean]\n",
       "                      ...               \n",
       "34884              [go, left, go, right]\n",
       "34885    [must, read, textbook, closely]\n",
       "34886                          [problem]\n",
       "34887                 [ve, got, problem]\n",
       "34888                 [ve, got, problem]\n",
       "Name: english_preprocessed, Length: 34889, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['english_preprocessed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76736404",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd6b2f",
   "metadata": {},
   "source": [
    "### word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ec2487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Assuming you have lists of tokenized Arabic and English sentences\n",
    "arabic_corpus = [sentence for sentence in dataset['arabic_preprocessed']]\n",
    "english_corpus = [sentence for sentence in dataset['english_preprocessed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f46e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Arabic Word2Vec model\n",
    "arabic_model = Word2Vec(sentences=arabic_corpus, vector_size=300, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Train English Word2Vec model\n",
    "english_model = Word2Vec(sentences=english_corpus, vector_size=300, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "588322e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_model.save('arabic_word2vec.model')\n",
    "english_model.save('english_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89becdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "arabic_model = Word2Vec.load('arabic_word2vec.model')\n",
    "english_model = Word2Vec.load('english_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf8d030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_to_embeddings(text, model):\n",
    "    embeddings = []\n",
    "    for word in text:\n",
    "        if word in model.wv:\n",
    "            embeddings.append(model.wv[word])\n",
    "        else:\n",
    "            # Handle out-of-vocabulary words\n",
    "            embeddings.append(np.zeros(model.vector_size))\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f49b06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['arabic_embeddings'] = dataset['arabic_preprocessed'].apply(lambda x: text_to_embeddings(x, arabic_model))\n",
    "dataset['english_embeddings'] = dataset['english_preprocessed'].apply(lambda x: text_to_embeddings(x, english_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16cc25da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [[0.012007201, 0.097810835, 0.02885719, 0.0256...\n",
       "1        [[0.006580001, 0.06165389, 0.021370022, 0.0136...\n",
       "2        [[0.036001354, 0.2164926, 0.066832535, 0.05173...\n",
       "3        [[0.030880434, 0.22037748, 0.06586347, 0.05477...\n",
       "4        [[0.029636184, 0.23839317, 0.0652243, 0.059855...\n",
       "                               ...                        \n",
       "34884    [[0.03176287189126015, 0.27656808495521545, 0....\n",
       "34885    [[0.038611628115177155, 0.28334930539131165, 0...\n",
       "34886    [[0.04234245, 0.29211202, 0.08000256, 0.069772...\n",
       "34887    [[0.04234245, 0.29211202, 0.08000256, 0.069772...\n",
       "34888    [[0.04234245, 0.29211202, 0.08000256, 0.069772...\n",
       "Name: arabic_embeddings, Length: 34889, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['arabic_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b82e4a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [[0.023564283, 0.21296181, -0.009773556, 0.106...\n",
       "1        [[0.017972827, 0.23550971, -0.0066297464, 0.11...\n",
       "2        [[0.027597813, 0.2323528, -0.018253095, 0.1107...\n",
       "3        [[0.010202153, 0.09688939, -0.0017658105, 0.05...\n",
       "4        [[0.010366553, 0.095281266, -0.0074263937, 0.0...\n",
       "                               ...                        \n",
       "34884    [[0.021633, 0.23766693, -0.022877734, 0.128194...\n",
       "34885    [[0.029085808, 0.25794262, -0.0073842155, 0.13...\n",
       "34886    [[0.022605492, 0.24313901, -0.0070547853, 0.12...\n",
       "34887    [[0.029859865, 0.24789649, -0.020183185, 0.137...\n",
       "34888    [[0.029859865, 0.24789649, -0.020183185, 0.137...\n",
       "Name: english_embeddings, Length: 34889, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['english_embeddings']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef0f1f",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba832f",
   "metadata": {},
   "source": [
    "### LSTM (seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a5fc3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8baa5bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">104</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">570,368</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "│                               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]        │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                 │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">570,368</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
       "│                               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]        │                 │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>], lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2646</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">680,022</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m104\u001b[0m, \u001b[38;5;34m300\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m89\u001b[0m, \u001b[38;5;34m300\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                   │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │         \u001b[38;5;34m570,368\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "│                               │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]        │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                 │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m89\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,  │         \u001b[38;5;34m570,368\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
       "│                               │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]        │                 │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m], lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m89\u001b[0m, \u001b[38;5;34m2646\u001b[0m)          │         \u001b[38;5;34m680,022\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,820,758</span> (6.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,820,758\u001b[0m (6.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,820,758</span> (6.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,820,758\u001b[0m (6.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the maximum sequence length for both input and output sequences\n",
    "max_encoder_seq_length = max(len(seq) for seq in dataset['arabic_preprocessed'])\n",
    "max_decoder_seq_length = max(len(seq) for seq in dataset['english_preprocessed'])\n",
    "\n",
    "# Define the input sequence\n",
    "encoder_inputs = Input(shape=(max_encoder_seq_length, arabic_model.vector_size))\n",
    "\n",
    "# Define the LSTM encoder\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the input sequence for the decoder\n",
    "decoder_inputs = Input(shape=(max_decoder_seq_length, english_model.vector_size))\n",
    "\n",
    "# Define the LSTM decoder\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# Define the Dense layer for output\n",
    "decoder_dense = Dense(len(english_model.wv.key_to_index), activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a317d44",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e766fb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset (assumes a CSV file with 'arabic' and 'english' columns)\n",
    "df = pd.read_csv(\"arabic_english_dataset.csv\")  # Replace with your dataset filename\n",
    "df = df.dropna()\n",
    "\n",
    "# Optional: Add <start> and <end> tokens to target (English) sentences\n",
    "df['english'] = df['english'].apply(lambda x: '<start> ' + x + ' <end>')\n",
    "\n",
    "# Split data\n",
    "input_texts = df['arabic'].values\n",
    "target_texts = df['english'].values\n",
    "\n",
    "# Tokenize Arabic\n",
    "arabic_tokenizer = Tokenizer()\n",
    "arabic_tokenizer.fit_on_texts(input_texts)\n",
    "input_sequences = arabic_tokenizer.texts_to_sequences(input_texts)\n",
    "input_sequences = pad_sequences(input_sequences, padding='post')\n",
    "\n",
    "# Tokenize English\n",
    "english_tokenizer = Tokenizer(filters='')\n",
    "english_tokenizer.fit_on_texts(target_texts)\n",
    "target_sequences = english_tokenizer.texts_to_sequences(target_texts)\n",
    "target_sequences = pad_sequences(target_sequences, padding='post')\n",
    "\n",
    "# Vocabulary sizes\n",
    "input_vocab_size = len(arabic_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(input_sequences, target_sequences, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c845809b",
   "metadata": {},
   "source": [
    "#  Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82797f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "\n",
    "embedding_dim = 256\n",
    "lstm_units = 512\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(lstm_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb = Embedding(target_vocab_size, embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = Dense(target_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Shift y_train by 1 timestep for decoder target\n",
    "y_train_shifted = np.expand_dims(y_train[:, 1:], -1)\n",
    "y_val_shifted = np.expand_dims(y_val[:, 1:], -1)\n",
    "\n",
    "# Train\n",
    "model.fit(\n",
    "    [X_train, y_train[:, :-1]],\n",
    "    y_train_shifted,\n",
    "    validation_data=([X_val, y_val[:, :-1]], np.expand_dims(y_val[:, 1:], -1)),\n",
    "    batch_size=64,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed38c3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
